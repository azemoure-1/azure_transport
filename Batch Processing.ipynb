{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48783816-0a0c-486d-934f-86195eccccc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset public_transport_data_month_9.csv copied to 'processed' folder.\n",
      "Imported dataset: public_transport_data_month_9.csv\n",
      "No more datasets to import.\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from pyspark.sql.functions import to_date, year, month, day, hour, minute, when, avg, regexp_replace, mean, count, round\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Mounting data lake\n",
    "storageAccountName = \"azemourstorage\"\n",
    "storageAccountAccessKey = \"kEjUrj5seRnPiSRkcQRPmozB610QhZLtZn9GrRhZk/D76nVsr6DNejvGRq0wEQWFiqcJjWgWi7cQ+AStydUbmQ==\"\n",
    "sasToken = \"?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2023-09-27T17:28:21Z&st=2023-09-27T09:28:21Z&spr=https&sig=EfKk2ZhRwqQFy228Dt7Z19oDuI61hNTNiQkAinm2nBc%3D\"\n",
    "blobContainerName = \"public-transport-data\"\n",
    "mountPoint = \"/mnt/public-transport-data/\"\n",
    "if not any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):\n",
    "  try:\n",
    "    dbutils.fs.mount(\n",
    "      source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n",
    "      mount_point = mountPoint,\n",
    "      extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n",
    "    )\n",
    "    print(\"mount succeeded!\")\n",
    "  except Exception as e:\n",
    "    print(\"mount exception\", e)\n",
    "\n",
    "# Define Azure Blob Storage paths\n",
    "raw_folder_path = \"/mnt/public-transport-data/raw/\"\n",
    "processed_folder_path = \"/mnt/public-transport-data/processed/\"\n",
    "csv_file_path = \"/mnt/public-transport-data/processed_files/processing_state.csv\"\n",
    "\n",
    "# Function to extract the month number from the filename\n",
    "def extract_month(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    for part in parts:\n",
    "        if part.isdigit():\n",
    "            return int(part)\n",
    "    return 0  # Return 0 if month number is not found\n",
    "\n",
    "# List to keep track of imported datasets\n",
    "imported_datasets = []\n",
    "\n",
    "# Load the existing dataset names from the CSV file\n",
    "existing_datasets = set()\n",
    "try:\n",
    "    # Read existing dataset names from the CSV file\n",
    "    existing_df = spark.read.option(\"header\", True).csv(csv_file_path)\n",
    "    existing_datasets = set(existing_df.select(\"name_file\").rdd.flatMap(lambda x: x).collect())\n",
    "except Exception as e:\n",
    "    print(\"Error reading existing datasets:\", str(e))\n",
    "\n",
    "# Get a list of files in the 'raw' folder in Azure Blob Storage\n",
    "files_in_raw_folder = dbutils.fs.ls(raw_folder_path)\n",
    "data_files = [file.name for file in files_in_raw_folder if file.isFile() and file.name not in existing_datasets]\n",
    "\n",
    "# Sort the data files by their numeric part (e.g., month)\n",
    "data_files.sort(key=lambda x: extract_month(x))\n",
    "\n",
    "# Function to import a new dataset\n",
    "def import_new_dataset():\n",
    "    if data_files:\n",
    "        # Get the next dataset to import\n",
    "        dataset_to_import = data_files.pop(0)\n",
    "        \n",
    "        # Check if dataset is already imported\n",
    "        if dataset_to_import in existing_datasets:\n",
    "            print(f\"Dataset {dataset_to_import} already imported. Skipping...\")\n",
    "        else:\n",
    "            # Update the list of imported datasets\n",
    "            imported_datasets.append(dataset_to_import)\n",
    "            \n",
    "            # Save dataset name and current date to the CSV file\n",
    "            save_to_csv(dataset_to_import, datetime.now().strftime('%Y-%m-%d'))\n",
    "            \n",
    "            # Copy the dataset to the 'processed' folder\n",
    "            copy_to_processed(dataset_to_import)\n",
    "            \n",
    "            # TODO: Import the dataset (use dataset_to_import)\n",
    "            # Replace the following print statement with your dataset import logic\n",
    "            print(f\"Imported dataset: {dataset_to_import}\")\n",
    "    else:\n",
    "        print(\"No more datasets to import.\")\n",
    "\n",
    "# Function to save dataset name and date to the CSV file\n",
    "def save_to_csv(dataset_name, import_date):\n",
    "    try:\n",
    "        # Create a DataFrame with the new dataset import record\n",
    "        new_record_df = spark.createDataFrame([(dataset_name, import_date)], [\"name_file\", \"date\"])\n",
    "        \n",
    "        # Append the new record to the existing CSV file\n",
    "        existing_df = spark.read.option(\"header\", True).csv(csv_file_path)\n",
    "        updated_df = existing_df.union(new_record_df)\n",
    "        updated_df.write.option(\"header\", True).mode(\"overwrite\").csv(csv_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error saving to CSV:\", str(e))\n",
    "\n",
    "# Function to copy the dataset to the 'processed' folder\n",
    "def copy_to_processed(dataset_name):\n",
    "    source_path = raw_folder_path + dataset_name\n",
    "    destination_path = processed_folder_path + dataset_name\n",
    "    try:\n",
    "        # Copy the dataset to the 'processed' folder\n",
    "        dbutils.fs.cp(source_path, destination_path)\n",
    "        print(f\"Dataset {dataset_name} copied to 'processed' folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy dataset {dataset_name} to 'processed' folder. Error: {str(e)}\")\n",
    "\n",
    "# Perform dataset imports in the dynamically generated order\n",
    "import_new_dataset()\n",
    "import_new_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e98e052-239c-4fce-8c6a-7fd2311c4d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset public_transport_data_month_10.csv copied to 'processed' folder.\n",
      "Imported dataset: public_transport_data_month_10.csv\n",
      "Dataset public_transport_data_month_11.csv copied to 'processed' folder.\n",
      "Imported dataset: public_transport_data_month_11.csv\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import to_date, year, month, day, hour, minute, when, avg, regexp_replace, mean, count, round\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define Azure Blob Storage paths\n",
    "raw_folder_path = \"/mnt/public-transport-data/raw/\"\n",
    "processed_folder_path = \"/mnt/public-transport-data/processed/\"\n",
    "csv_file_path = \"/mnt/public-transport-data/processed_fils/processing_state.csv\"\n",
    "\n",
    "# Define the schema for the CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"name_file\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Function to extract the month number from the filename\n",
    "def extract_month(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    for part in parts:\n",
    "        if part.isdigit():\n",
    "            return int(part)\n",
    "    return 0  # Return 0 if month number is not found\n",
    "\n",
    "# List to keep track of imported datasets\n",
    "imported_datasets = []\n",
    "\n",
    "# Load the existing dataset names from the CSV file\n",
    "existing_datasets = set()\n",
    "try:\n",
    "    # Read existing dataset names from the CSV file\n",
    "    existing_df = spark.read.option(\"header\", True).schema(schema).csv(csv_file_path)\n",
    "    existing_datasets = set(existing_df.select(\"name_file\").rdd.flatMap(lambda x: x).collect())\n",
    "except Exception as e:\n",
    "    print(\"Error reading existing datasets:\", str(e))\n",
    "\n",
    "# Get a list of files in the 'raw' folder in Azure Blob Storage\n",
    "files_in_raw_folder = dbutils.fs.ls(raw_folder_path)\n",
    "data_files = [file.name for file in files_in_raw_folder if file.isFile() and file.name not in existing_datasets]\n",
    "\n",
    "# Sort the data files by their numeric part (e.g., month)\n",
    "data_files.sort(key=lambda x: extract_month(x))\n",
    "\n",
    "# Function to import a new dataset\n",
    "def import_new_dataset():\n",
    "    if data_files:\n",
    "        # Get the next dataset to import\n",
    "        dataset_to_import = data_files.pop(0)\n",
    "        \n",
    "        # Check if dataset is already imported\n",
    "        if dataset_to_import in existing_datasets:\n",
    "            print(f\"Dataset {dataset_to_import} already imported. Skipping...\")\n",
    "        else:\n",
    "            # Update the list of imported datasets\n",
    "            imported_datasets.append(dataset_to_import)\n",
    "            \n",
    "            # Save dataset name and current date to the CSV file\n",
    "            save_to_csv(dataset_to_import, datetime.now().strftime('%Y-%m-%d'))\n",
    "            \n",
    "            # Copy the dataset to the 'processed' folder\n",
    "            copy_to_processed(dataset_to_import)\n",
    "            \n",
    "            # TODO: Import the dataset (use dataset_to_import)\n",
    "            # Replace the following print statement with your dataset import logic\n",
    "            print(f\"Imported dataset: {dataset_to_import}\")\n",
    "    else:\n",
    "        print(\"No more datasets to import.\")\n",
    "\n",
    "# Function to save dataset name and date to the CSV file\n",
    "def save_to_csv(dataset_name, import_date):\n",
    "    try:\n",
    "        # Create a DataFrame with the new dataset import record\n",
    "        new_record_df = spark.createDataFrame([(dataset_name, import_date)], schema)\n",
    "        \n",
    "        # Read existing dataset names and dates from the CSV file\n",
    "        existing_df = spark.read.option(\"header\", True).schema(schema).csv(csv_file_path)\n",
    "        \n",
    "        # Union the existing dataset with the new record\n",
    "        updated_df = existing_df.union(new_record_df)\n",
    "        \n",
    "        # Write the updated DataFrame to the CSV file\n",
    "        updated_df.write.option(\"header\", True).mode(\"overwrite\").csv(csv_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error saving to CSV:\", str(e))\n",
    "\n",
    "# Function to copy the dataset to the 'processed' folder\n",
    "def copy_to_processed(dataset_name):\n",
    "    source_path = raw_folder_path + dataset_name\n",
    "    destination_path = processed_folder_path + dataset_name\n",
    "    try:\n",
    "        # Copy the dataset to the 'processed' folder\n",
    "        dbutils.fs.cp(source_path, destination_path)\n",
    "        print(f\"Dataset {dataset_name} copied to 'processed' folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy dataset {dataset_name} to 'processed' folder. Error: {str(e)}\")\n",
    "\n",
    "# Perform dataset imports in the dynamically generated order\n",
    "import_new_dataset()\n",
    "import_new_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4c3ca0-359c-4487-b193-826d4b1978e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying dataset: public_transport_data_month_11.csv\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "|      Date|TransportType|   Route|DepartureTime|ArrivalTime|Passengers|DepartureStation|ArrivalStation|Delay|DurationSeconds|Duration|DelayCategory|Hour| HourCategory|\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "|2023-11-01|          Bus| Route_4|     14:41:00|   15:10:00|        49|      Station_10|     Station_8|   13|           1740|00:29:00| Retard Moyen|  14|Off-Peak Hour|\n",
      "|2023-11-01|        Metro| Route_4|     12:12:00|   13:24:00|        18|      Station_14|     Station_3|    6|           4320|01:12:00| Retard Court|  12|Off-Peak Hour|\n",
      "|2023-11-01|        Metro| Route_2|     13:17:00|   13:37:00|        30|      Station_13|    Station_14|    6|           1200|00:20:00| Retard Court|  13|Off-Peak Hour|\n",
      "|2023-11-01|        Train| Route_4|     18:39:00|   19:55:00|        50|       Station_7|     Station_8|    4|           4560|01:16:00| Retard Court|  18|Off-Peak Hour|\n",
      "|2023-11-01|         Tram| Route_6|     19:42:00|   19:11:00|        77|      Station_17|    Station_10|    8|          -1860|23:29:00| Retard Court|  19|    Peak Hour|\n",
      "|2023-11-01|        Train| Route_8|     11:47:00|   11:55:00|        21|       Station_6|    Station_20|    2|            480|00:08:00| Retard Court|  11|Off-Peak Hour|\n",
      "|2023-11-01|        Metro| Route_8|     12:40:00|   13:59:00|        53|      Station_12|    Station_10|   11|           4740|01:19:00| Retard Moyen|  12|Off-Peak Hour|\n",
      "|2023-11-01|        Metro| Route_3|     12:40:00|   12:51:00|        25|      Station_17|    Station_16|    3|            660|00:11:00| Retard Court|  12|Off-Peak Hour|\n",
      "|2023-11-01|         Tram| Route_7|     13:54:00|   13:35:00|        37|      Station_19|    Station_19|    1|          -1140|23:41:00| Retard Court|  13|Off-Peak Hour|\n",
      "|2023-11-01|        Metro| Route_8|     06:47:00|   07:15:00|         1|       Station_1|    Station_12|    6|           1680|00:28:00| Retard Court|   6|Off-Peak Hour|\n",
      "|2023-11-01|         Tram|Route_10|     15:40:00|   15:31:00|        77|      Station_17|     Station_1|    7|           -540|23:51:00| Retard Court|  15|    Peak Hour|\n",
      "|2023-11-01|        Train| Route_3|     12:11:00|   12:24:00|        25|       Station_1|    Station_14|    4|            780|00:13:00| Retard Court|  12|Off-Peak Hour|\n",
      "|2023-11-01|        Train|Route_10|     17:24:00|   19:07:00|        52|      Station_20|    Station_18|   15|           6180|01:43:00| Retard Moyen|  17|Off-Peak Hour|\n",
      "|2023-11-01|        Train| Route_4|     22:46:00|   23:53:00|        38|      Station_15|    Station_11|    1|           4020|01:07:00| Retard Court|  22|Off-Peak Hour|\n",
      "|2023-11-01|          Bus| Route_6|     17:28:00|   18:48:00|        33|      Station_20|    Station_15|    1|           4800|01:20:00| Retard Court|  17|Off-Peak Hour|\n",
      "|2023-11-01|        Train| Route_9|     08:38:00|   09:45:00|        61|       Station_1|     Station_6|   12|           4020|01:07:00| Retard Moyen|   8|Off-Peak Hour|\n",
      "|2023-11-01|        Metro|Route_10|     06:08:00|   07:25:00|        82|       Station_6|     Station_6|   13|           4620|01:17:00| Retard Moyen|   6|    Peak Hour|\n",
      "|2023-11-01|        Train| Route_1|     09:44:00|   09:24:00|        20|      Station_12|    Station_16|    7|          -1200|23:40:00| Retard Court|   9|Off-Peak Hour|\n",
      "|2023-11-01|        Metro| Route_3|     12:38:00|   13:07:00|        30|      Station_13|    Station_10|    7|           1740|00:29:00| Retard Court|  12|Off-Peak Hour|\n",
      "|2023-11-01|          Bus| Route_6|     22:08:00|   23:05:00|        13|      Station_15|    Station_18|    3|           3420|00:57:00| Retard Court|  22|Off-Peak Hour|\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Failed to copy dataset public_transport_data_month_11.csv to 'processed' folder. Error: name 'dataset_df' is not defined\n",
      "Imported dataset: public_transport_data_month_11.csv\n",
      "Displaying dataset: public_transport_data_month_3.csv\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "|      Date|TransportType|   Route|DepartureTime|ArrivalTime|Passengers|DepartureStation|ArrivalStation|Delay|DurationSeconds|Duration|DelayCategory|Hour| HourCategory|\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "|2023-03-01|        Train|Route_10|     13:07:00|   13:55:00|        16|      Station_17|    Station_17|   15|           2880|00:48:00| Retard Moyen|  13|Off-Peak Hour|\n",
      "|2023-03-01|          Bus| Route_9|     09:36:00|   10:16:00|        23|      Station_15|    Station_11|    5|           2400|00:40:00| Retard Court|   9|Off-Peak Hour|\n",
      "|2023-03-01|        Train| Route_1|     22:10:00|   23:12:00|        70|       Station_7|    Station_18|    7|           3720|01:02:00| Retard Court|  22|    Peak Hour|\n",
      "|2023-03-01|        Metro| Route_3|     19:29:00|   21:14:00|        13|       Station_4|     Station_4|   15|           6300|01:45:00| Retard Moyen|  19|Off-Peak Hour|\n",
      "|2023-03-01|         Tram|Route_10|     10:27:00|   11:42:00|        53|       Station_5|    Station_11|    6|           4500|01:15:00| Retard Court|  10|Off-Peak Hour|\n",
      "|2023-03-01|        Train| Route_1|     21:57:00|   23:34:00|         9|      Station_15|     Station_8|    9|           5820|01:37:00| Retard Court|  21|Off-Peak Hour|\n",
      "|2023-03-01|        Metro| Route_7|     22:52:00|   01:01:00|        79|      Station_10|     Station_5|    6|         -78660|02:09:00| Retard Court|  22|    Peak Hour|\n",
      "|2023-03-01|        Train| Route_8|     13:19:00|   13:53:00|        94|      Station_19|     Station_1|   15|           2040|00:34:00| Retard Moyen|  13|    Peak Hour|\n",
      "|2023-03-01|        Train|Route_10|     10:47:00|   12:56:00|        40|      Station_14|     Station_6|   15|           7740|02:09:00| Retard Moyen|  10|Off-Peak Hour|\n",
      "|2023-03-01|          Bus| Route_6|     06:58:00|   08:52:00|         6|       Station_5|    Station_10|   10|           6840|01:54:00| Retard Court|   6|Off-Peak Hour|\n",
      "|2023-03-01|        Metro| Route_1|     12:02:00|   12:28:00|        33|      Station_18|     Station_4|   10|           1560|00:26:00| Retard Court|  12|Off-Peak Hour|\n",
      "|2023-03-01|        Train|Route_10|     05:24:00|   06:01:00|        32|      Station_13|    Station_19|    3|           2220|00:37:00| Retard Court|   5|Off-Peak Hour|\n",
      "|2023-03-01|          Bus| Route_4|     08:56:00|   10:31:00|        25|       Station_6|     Station_5|    1|           5700|01:35:00| Retard Court|   8|Off-Peak Hour|\n",
      "|2023-03-01|        Metro| Route_2|     18:26:00|   18:53:00|        77|      Station_11|    Station_10|    3|           1620|00:27:00| Retard Court|  18|    Peak Hour|\n",
      "|2023-03-01|        Metro| Route_7|     13:57:00|   14:28:00|        29|      Station_16|     Station_3|    3|           1860|00:31:00| Retard Court|  13|Off-Peak Hour|\n",
      "|2023-03-01|        Train| Route_7|     16:56:00|   18:45:00|        35|      Station_14|     Station_6|    7|           6540|01:49:00| Retard Court|  16|Off-Peak Hour|\n",
      "|2023-03-01|        Metro| Route_1|     08:16:00|   08:53:00|        76|      Station_11|    Station_17|    4|           2220|00:37:00| Retard Court|   8|    Peak Hour|\n",
      "|2023-03-01|        Metro| Route_9|     05:27:00|   06:11:00|        39|       Station_6|    Station_15|   13|           2640|00:44:00| Retard Moyen|   5|Off-Peak Hour|\n",
      "|2023-03-01|        Metro| Route_9|     14:33:00|   15:59:00|         9|      Station_15|     Station_4|    7|           5160|01:26:00| Retard Court|  14|Off-Peak Hour|\n",
      "|2023-03-01|        Train| Route_2|     11:31:00|   13:36:00|        96|       Station_3|     Station_9|   15|           7500|02:05:00| Retard Moyen|  11|    Peak Hour|\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Failed to copy dataset public_transport_data_month_3.csv to 'processed' folder. Error: name 'dataset_df' is not defined\n",
      "Imported dataset: public_transport_data_month_3.csv\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import to_date, year, month, day, hour, minute, when, avg, regexp_replace, mean, count, round\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import col, regexp_extract, to_timestamp\n",
    "from pyspark.sql.functions import unix_timestamp, col, expr\n",
    "\n",
    "# Define Azure Blob Storage paths\n",
    "raw_folder_path = \"/mnt/public-transport-data/raw/\"\n",
    "processed_folder_path = \"/mnt/public-transport-data/processed/\"\n",
    "csv_file_path = \"/mnt/public-transport-data/processed_fils/\"\n",
    "\n",
    "# Define the schema for the CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"name_file\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Function to extract the month number from the filename\n",
    "def extract_month(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    for part in parts:\n",
    "        if part.isdigit():\n",
    "            return int(part)\n",
    "    return 0  # Return 0 if month number is not found\n",
    "\n",
    "\n",
    "# Load the existing dataset names from the CSV file\n",
    "existing_datasets = set()\n",
    "try:\n",
    "    # Read existing dataset names from the CSV file\n",
    "    existing_df = spark.read.option(\"header\", True).schema(schema).csv(csv_file_path)\n",
    "    existing_datasets = set(existing_df.select(\"name_file\").rdd.flatMap(lambda x: x).collect())\n",
    "except Exception as e:\n",
    "    print(\"Error reading existing datasets:\", str(e))\n",
    "\n",
    "# Get a list of files in the 'raw' folder in Azure Blob Storage\n",
    "files_in_raw_folder = dbutils.fs.ls(raw_folder_path)\n",
    "data_files = [file.name for file in files_in_raw_folder if file.isFile() and file.name not in existing_datasets]\n",
    "\n",
    "# Sort the data files by their numeric part (e.g., month)\n",
    "data_files.sort(key=lambda x: extract_month(x))\n",
    "\n",
    "# Function to import a new dataset\n",
    "def import_new_dataset():\n",
    "    if data_files:\n",
    "        # Get the next dataset to import\n",
    "        dataset_to_import = data_files.pop(0)\n",
    "        \n",
    "        # Check if dataset is already imported\n",
    "        if dataset_to_import in existing_datasets:\n",
    "            print(f\"Dataset {dataset_to_import} already imported. Skipping...\")\n",
    "        else:\n",
    "            # Update the list of imported datasets\n",
    "            imported_datasets.append(dataset_to_import)\n",
    "            \n",
    "            # Save dataset name and current date to the CSV file\n",
    "            save_to_csv(dataset_to_import, datetime.now().strftime('%Y-%m-%d'))\n",
    "            \n",
    "            # Display the dataset (use dataset_to_import)\n",
    "            print(f\"Displaying dataset: {dataset_to_import}\")\n",
    "            dataset_df = spark.read.option(\"header\", True).csv(raw_folder_path + dataset_to_import)\n",
    "            \n",
    "            def convert_to_24_hour(time_str):\n",
    "                return to_timestamp(time_str, 'h:mm:ss a').cast('string')\n",
    "\n",
    "            # Apply the function to DepartureTime and ArrivalTime columns\n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', regexp_extract(col('DepartureTime'), r'(\\d+:\\d+:\\d+ [APM]+)',1))\n",
    "                          .withColumn('ArrivalTime', regexp_extract(col('ArrivalTime'), r'(\\d+:\\d+:\\d+ [APM]+)', 1)))\n",
    "\n",
    "            # Convert DepartureTime and ArrivalTime to 24-hour format\n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', convert_to_24_hour(col('DepartureTime')))\n",
    "                         .withColumn('ArrivalTime', convert_to_24_hour(col('ArrivalTime'))))\n",
    "            \n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', date_format(col('DepartureTime'), 'HH:mm:ss'))\n",
    "                         .withColumn('ArrivalTime', date_format(col('ArrivalTime'), 'HH:mm:ss')))\n",
    "            \n",
    "            # Calculate the duration in seconds\n",
    "            dataset_df = dataset_df.withColumn('DurationSeconds', \n",
    "                                                (unix_timestamp(col('ArrivalTime'), 'HH:mm:ss') - \n",
    "                                                unix_timestamp(col('DepartureTime'), 'HH:mm:ss')))\n",
    "\n",
    "            # Convert the duration from seconds to HH:mm:ss format\n",
    "            dataset_df = dataset_df.withColumn('Duration', \n",
    "                                                expr(\"from_unixtime(DurationSeconds, 'HH:mm:ss')\"))\n",
    "            \n",
    "            dataset_df = dataset_df.withColumn('DelayCategory',\n",
    "                                        when(col('Delay') == 0, 'Pas de Retard')\n",
    "                                        .when((col('Delay') >= 1) & (col('Delay') <= 10), 'Retard Court')\n",
    "                                        .when((col('Delay') >= 11) & (col('Delay') <= 20), 'Retard Moyen')\n",
    "                                        .when(col('Delay') > 20, 'Long Retard')\n",
    "                                        .otherwise('Unknown'))\n",
    "            \n",
    "\n",
    "            peak_passenger_threshold = 70\n",
    "\n",
    "            # Identify peak and off-peak hours based on the number of passengers\n",
    "            dataset_df = (dataset_df.withColumn('Hour', hour(col('DepartureTime')))\n",
    "                                      .withColumn('HourCategory',\n",
    "                                                  when(col('Passengers') >= peak_passenger_threshold, 'Peak Hour')\n",
    "                                                  .otherwise('Off-Peak Hour')))\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            dataset_df.show()  # Show the contents of the dataset\n",
    "            \n",
    "            # Copy the dataset to the 'processed' folder\n",
    "            copy_to_processed(dataset_to_import)\n",
    "            \n",
    "            # TODO: Import the dataset (use dataset_to_import)\n",
    "            # Replace the following print statement with your dataset import logic\n",
    "            print(f\"Imported dataset: {dataset_to_import}\")\n",
    "    else:\n",
    "        print(\"No more datasets to import.\")\n",
    "\n",
    "# Function to save dataset name and date to the CSV file\n",
    "def save_to_csv(dataset_name, import_date):\n",
    "    try:\n",
    "        # Create a DataFrame with the new dataset import record\n",
    "        new_record_df = spark.createDataFrame([(dataset_name, import_date)], schema)\n",
    "        \n",
    "        # Read existing dataset names and dates from the CSV file\n",
    "        existing_df = spark.read.option(\"header\", True).schema(schema).csv(csv_file_path)\n",
    "        \n",
    "        # Union the existing dataset with the new record\n",
    "        updated_df = existing_df.union(new_record_df)\n",
    "        \n",
    "        # Write the updated DataFrame to the CSV file\n",
    "        updated_df.write.option(\"header\", True).mode(\"overwrite\").csv(csv_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error saving to CSV:\", str(e))\n",
    "\n",
    "# Function to copy the dataset to the 'processed' folder\n",
    "def copy_to_processed(dataset_name):\n",
    "    source_path = raw_folder_path + dataset_name\n",
    "    destination_path = processed_folder_path + dataset_name\n",
    "    try:\n",
    "        # Copy the dataset to the 'processed' folder\n",
    "        dbutils.fs.cp(source_path, destination_path)\n",
    "        print(f\"Dataset {dataset_name} copied to 'processed' folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy dataset {dataset_name} to 'processed' folder. Error: {str(e)}\")\n",
    "\n",
    "# Perform dataset imports in the dynamically generated order\n",
    "import_new_dataset()\n",
    "import_new_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a62c64-8eb7-4fd5-8a29-ae05dbcecaeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying dataset: public_transport_data_month_4.csv\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "|      Date|TransportType|   Route|DepartureTime|ArrivalTime|Passengers|DepartureStation|ArrivalStation|Delay|DurationSeconds|Duration|DelayCategory|Hour| HourCategory|\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "|2023-04-01|          Bus| Route_6|     10:05:00|   11:20:00|        65|       Station_9|     Station_9|   12|           4500|01:15:00| Retard Moyen|  10|Off-Peak Hour|\n",
      "|2023-04-01|        Metro| Route_6|     08:03:00|   10:05:00|        30|      Station_11|     Station_7|    4|           7320|02:02:00| Retard Court|   8|Off-Peak Hour|\n",
      "|2023-04-01|        Metro| Route_3|     07:30:00|   09:32:00|        58|      Station_17|    Station_11|    1|           7320|02:02:00| Retard Court|   7|Off-Peak Hour|\n",
      "|2023-04-01|         Tram| Route_6|     13:55:00|   14:29:00|        40|       Station_9|     Station_5|    7|           2040|00:34:00| Retard Court|  13|Off-Peak Hour|\n",
      "|2023-04-01|         Tram| Route_1|     15:12:00|   16:59:00|        74|       Station_9|    Station_17|   14|           6420|01:47:00| Retard Moyen|  15|    Peak Hour|\n",
      "|2023-04-01|          Bus| Route_1|     15:56:00|   17:05:00|        55|       Station_2|    Station_15|   13|           4140|01:09:00| Retard Moyen|  15|Off-Peak Hour|\n",
      "|2023-04-01|        Train| Route_6|     08:14:00|   09:14:00|        36|      Station_13|    Station_14|    5|           3600|01:00:00| Retard Court|   8|Off-Peak Hour|\n",
      "|2023-04-01|        Metro|Route_10|     12:19:00|   14:16:00|        39|      Station_17|     Station_8|   14|           7020|01:57:00| Retard Moyen|  12|Off-Peak Hour|\n",
      "|2023-04-01|          Bus| Route_9|     13:40:00|   15:48:00|        44|      Station_19|     Station_1|    9|           7680|02:08:00| Retard Court|  13|Off-Peak Hour|\n",
      "|2023-04-01|         Tram| Route_8|     07:59:00|   09:24:00|         4|      Station_15|     Station_8|   14|           5100|01:25:00| Retard Moyen|   7|Off-Peak Hour|\n",
      "|2023-04-01|        Train| Route_2|     06:51:00|   08:15:00|        60|      Station_20|     Station_9|    4|           5040|01:24:00| Retard Court|   6|Off-Peak Hour|\n",
      "|2023-04-01|        Train|Route_10|     21:56:00|   23:44:00|        60|       Station_7|     Station_9|   10|           6480|01:48:00| Retard Court|  21|Off-Peak Hour|\n",
      "|2023-04-01|         Tram| Route_4|     10:18:00|   12:09:00|        53|      Station_14|    Station_20|    8|           6660|01:51:00| Retard Court|  10|Off-Peak Hour|\n",
      "|2023-04-01|        Metro| Route_7|     18:43:00|   19:19:00|        79|       Station_9|    Station_13|   15|           2160|00:36:00| Retard Moyen|  18|    Peak Hour|\n",
      "|2023-04-01|        Train| Route_2|     17:18:00|   18:40:00|        32|      Station_12|     Station_9|    0|           4920|01:22:00|Pas de Retard|  17|Off-Peak Hour|\n",
      "|2023-04-01|          Bus| Route_5|     11:45:00|   13:43:00|        45|       Station_6|     Station_4|    7|           7080|01:58:00| Retard Court|  11|Off-Peak Hour|\n",
      "|2023-04-01|          Bus| Route_1|     07:40:00|   09:31:00|        30|       Station_7|     Station_7|    9|           6660|01:51:00| Retard Court|   7|Off-Peak Hour|\n",
      "|2023-04-01|          Bus| Route_7|     05:17:00|   07:06:00|        62|       Station_1|     Station_2|    9|           6540|01:49:00| Retard Court|   5|Off-Peak Hour|\n",
      "|2023-04-01|        Metro|Route_10|     20:55:00|   22:43:00|        34|      Station_17|    Station_19|    5|           6480|01:48:00| Retard Court|  20|Off-Peak Hour|\n",
      "|2023-04-01|         Tram| Route_2|     08:16:00|   08:59:00|         7|       Station_6|    Station_14|    5|           2580|00:43:00| Retard Court|   8|Off-Peak Hour|\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Dataset public_transport_data_month_4.csv copied to 'processed' folder.\n",
      "Imported dataset: public_transport_data_month_4.csv\n",
      "Displaying dataset: public_transport_data_month_5.csv\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "|      Date|TransportType|   Route|DepartureTime|ArrivalTime|Passengers|DepartureStation|ArrivalStation|Delay|DurationSeconds|Duration|DelayCategory|Hour| HourCategory|\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "|2023-05-01|        Metro| Route_5|     09:04:00|   10:24:00|        31|      Station_13|    Station_10|    2|           4800|01:20:00| Retard Court|   9|Off-Peak Hour|\n",
      "|2023-05-01|        Metro| Route_6|     07:33:00|   08:08:00|        48|      Station_16|     Station_9|    1|           2100|00:35:00| Retard Court|   7|Off-Peak Hour|\n",
      "|2023-05-01|         Tram| Route_2|     11:49:00|   11:54:00|        76|       Station_9|     Station_1|    5|            300|00:05:00| Retard Court|  11|    Peak Hour|\n",
      "|2023-05-01|        Train| Route_9|     20:59:00|   21:23:00|        72|      Station_18|    Station_15|    1|           1440|00:24:00| Retard Court|  20|    Peak Hour|\n",
      "|2023-05-01|         Tram| Route_1|     21:43:00|   22:04:00|        34|       Station_5|    Station_13|   10|           1260|00:21:00| Retard Court|  21|Off-Peak Hour|\n",
      "|2023-05-01|          Bus| Route_8|     08:22:00|   09:31:00|        16|      Station_14|     Station_6|   11|           4140|01:09:00| Retard Moyen|   8|Off-Peak Hour|\n",
      "|2023-05-01|        Train| Route_2|     10:47:00|   11:47:00|        17|      Station_17|     Station_7|   11|           3600|01:00:00| Retard Moyen|  10|Off-Peak Hour|\n",
      "|2023-05-01|        Metro| Route_7|     07:18:00|   08:35:00|        43|       Station_5|    Station_11|    9|           4620|01:17:00| Retard Court|   7|Off-Peak Hour|\n",
      "|2023-05-01|         Tram| Route_8|     09:36:00|   11:42:00|        57|      Station_12|    Station_15|    8|           7560|02:06:00| Retard Court|   9|Off-Peak Hour|\n",
      "|2023-05-01|         Tram| Route_4|     20:55:00|   22:08:00|        97|      Station_20|    Station_11|    0|           4380|01:13:00|Pas de Retard|  20|    Peak Hour|\n",
      "|2023-05-01|        Train|Route_10|     22:46:00|   23:54:00|        92|       Station_6|    Station_14|   12|           4080|01:08:00| Retard Moyen|  22|    Peak Hour|\n",
      "|2023-05-01|         Tram| Route_2|     18:53:00|   20:23:00|        65|       Station_1|     Station_8|    7|           5400|01:30:00| Retard Court|  18|Off-Peak Hour|\n",
      "|2023-05-01|        Metro|Route_10|     12:32:00|   13:48:00|        13|      Station_12|    Station_14|    5|           4560|01:16:00| Retard Court|  12|Off-Peak Hour|\n",
      "|2023-05-01|          Bus| Route_4|     22:23:00|   00:23:00|        18|      Station_17|    Station_17|    5|         -79200|02:00:00| Retard Court|  22|Off-Peak Hour|\n",
      "|2023-05-01|          Bus| Route_9|     11:55:00|   13:12:00|        37|      Station_10|     Station_8|    3|           4620|01:17:00| Retard Court|  11|Off-Peak Hour|\n",
      "|2023-05-01|          Bus| Route_9|     17:52:00|   19:38:00|         2|      Station_10|    Station_11|    6|           6360|01:46:00| Retard Court|  17|Off-Peak Hour|\n",
      "|2023-05-01|        Metro| Route_1|     08:07:00|   10:13:00|        35|       Station_2|    Station_19|    9|           7560|02:06:00| Retard Court|   8|Off-Peak Hour|\n",
      "|2023-05-01|        Metro| Route_3|     22:34:00|   00:04:00|         1|       Station_4|     Station_3|   13|         -81000|01:30:00| Retard Moyen|  22|Off-Peak Hour|\n",
      "|2023-05-01|         Tram| Route_4|     18:59:00|   19:37:00|        98|      Station_20|    Station_20|    9|           2280|00:38:00| Retard Court|  18|    Peak Hour|\n",
      "|2023-05-01|         Tram| Route_9|     11:31:00|   12:03:00|        49|       Station_3|     Station_3|    7|           1920|00:32:00| Retard Court|  11|Off-Peak Hour|\n",
      "+----------+-------------+--------+-------------+-----------+----------+----------------+--------------+-----+---------------+--------+-------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Dataset public_transport_data_month_5.csv copied to 'processed' folder.\n",
      "Imported dataset: public_transport_data_month_5.csv\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import to_date, year, month, day, hour, minute, when, avg, regexp_replace, mean, count, round\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import col, regexp_extract, to_timestamp\n",
    "from pyspark.sql.functions import unix_timestamp, col, expr\n",
    "\n",
    "# Define Azure Blob Storage paths\n",
    "raw_folder_path = \"/mnt/public-transport-data/raw/\"\n",
    "processed_folder_path = \"/mnt/public-transport-data/processed/\"\n",
    "csv_file_path = \"/mnt/public-transport-data/processed_fils/\"\n",
    "\n",
    "# Define the schema for the CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"name_file\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Function to extract the month number from the filename\n",
    "def extract_month(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    for part in parts:\n",
    "        if part.isdigit():\n",
    "            return int(part)\n",
    "    return 0  # Return 0 if month number is not found\n",
    "\n",
    "# Load the existing dataset names from the CSV file\n",
    "existing_datasets = set()\n",
    "try:\n",
    "    # Read existing dataset names from the CSV file\n",
    "    existing_df = spark.read.option(\"header\", True).schema(schema).csv(csv_file_path)\n",
    "    existing_datasets = set(existing_df.select(\"name_file\").rdd.flatMap(lambda x: x).collect())\n",
    "except Exception as e:\n",
    "    print(\"Error reading existing datasets:\", str(e))\n",
    "\n",
    "# Get a list of files in the 'raw' folder in Azure Blob Storage\n",
    "files_in_raw_folder = dbutils.fs.ls(raw_folder_path)\n",
    "data_files = [file.name for file in files_in_raw_folder if file.isFile() and file.name not in existing_datasets]\n",
    "\n",
    "# Sort the data files by their numeric part (e.g., month)\n",
    "data_files.sort(key=lambda x: extract_month(x))\n",
    "\n",
    "# Function to import a new dataset\n",
    "def import_new_dataset():\n",
    "    if data_files:\n",
    "        # Get the next dataset to import\n",
    "        dataset_to_import = data_files.pop(0)\n",
    "        \n",
    "        # Check if dataset is already imported\n",
    "        if dataset_to_import in existing_datasets:\n",
    "            print(f\"Dataset {dataset_to_import} already imported. Skipping...\")\n",
    "        else:\n",
    "            # Update the list of imported datasets\n",
    "            imported_datasets.append(dataset_to_import)\n",
    "            \n",
    "            # Save dataset name and current date to the CSV file\n",
    "            save_to_csv(dataset_to_import, datetime.now().strftime('%Y-%m-%d'))\n",
    "            \n",
    "            # Display the dataset (use dataset_to_import)\n",
    "            print(f\"Displaying dataset: {dataset_to_import}\")\n",
    "            dataset_df = spark.read.option(\"header\", True).csv(raw_folder_path + dataset_to_import)\n",
    "            \n",
    "            def convert_to_24_hour(time_str):\n",
    "                return to_timestamp(time_str, 'h:mm:ss a').cast('string')\n",
    "\n",
    "            # Apply the function to DepartureTime and ArrivalTime columns\n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', regexp_extract(col('DepartureTime'), r'(\\d+:\\d+:\\d+ [APM]+)',1))\n",
    "                          .withColumn('ArrivalTime', regexp_extract(col('ArrivalTime'), r'(\\d+:\\d+:\\d+ [APM]+)', 1)))\n",
    "\n",
    "            # Convert DepartureTime and ArrivalTime to 24-hour format\n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', convert_to_24_hour(col('DepartureTime')))\n",
    "                         .withColumn('ArrivalTime', convert_to_24_hour(col('ArrivalTime'))))\n",
    "            \n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', date_format(col('DepartureTime'), 'HH:mm:ss'))\n",
    "                         .withColumn('ArrivalTime', date_format(col('ArrivalTime'), 'HH:mm:ss')))\n",
    "            \n",
    "            # Calculate the duration in seconds\n",
    "            dataset_df = dataset_df.withColumn('DurationSeconds', \n",
    "                                                (unix_timestamp(col('ArrivalTime'), 'HH:mm:ss') - \n",
    "                                                unix_timestamp(col('DepartureTime'), 'HH:mm:ss')))\n",
    "\n",
    "            # Convert the duration from seconds to HH:mm:ss format\n",
    "            dataset_df = dataset_df.withColumn('Duration', \n",
    "                                                expr(\"from_unixtime(DurationSeconds, 'HH:mm:ss')\"))\n",
    "            \n",
    "            dataset_df = dataset_df.withColumn('DelayCategory',\n",
    "                                        when(col('Delay') == 0, 'Pas de Retard')\n",
    "                                        .when((col('Delay') >= 1) & (col('Delay') <= 10), 'Retard Court')\n",
    "                                        .when((col('Delay') >= 11) & (col('Delay') <= 20), 'Retard Moyen')\n",
    "                                        .when(col('Delay') > 20, 'Long Retard')\n",
    "                                        .otherwise('Unknown'))\n",
    "            \n",
    "\n",
    "            peak_passenger_threshold = 70\n",
    "\n",
    "            # Identify peak and off-peak hours based on the number of passengers\n",
    "            dataset_df = (dataset_df.withColumn('Hour', hour(col('DepartureTime')))\n",
    "                                      .withColumn('HourCategory',\n",
    "                                                  when(col('Passengers') >= peak_passenger_threshold, 'Peak Hour')\n",
    "                                                  .otherwise('Off-Peak Hour')))\n",
    "            \n",
    "            route_analysis = (dataset_df.groupBy('Route')\n",
    "                                .agg(avg('Delay').alias('AvgDelay'),\n",
    "                                     avg('Passengers').alias('AvgPassengers'),\n",
    "                                     count('*').alias('TotalJourneys')))\n",
    "\n",
    "            # Show the DataFrame with route analysis\n",
    "            route_analysis.show()\n",
    "\n",
    "            dataset_df.show()  # Show the contents of the dataset\n",
    "            \n",
    "            # Copy the dataset to the 'processed' folder\n",
    "            export_to_processed(dataset_to_import, dataset_df)\n",
    "            \n",
    "            print(f\"Imported dataset: {dataset_to_import}\")\n",
    "    else:\n",
    "        print(\"No more datasets to import.\")\n",
    "\n",
    "# Function to save dataset name and date to the CSV file\n",
    "def save_to_csv(dataset_name, import_date):\n",
    "    try:\n",
    "        # Create a DataFrame with the new dataset import record\n",
    "        new_record_df = spark.createDataFrame([(dataset_name, import_date)], schema)\n",
    "        \n",
    "        # Read existing dataset names and dates from the CSV file\n",
    "        existing_df = spark.read.option(\"header\", True).schema(schema).csv(csv_file_path)\n",
    "        \n",
    "        # Union the existing dataset with the new record\n",
    "        updated_df = existing_df.union(new_record_df)\n",
    "        \n",
    "        # Write the updated DataFrame to the CSV file\n",
    "        updated_df.write.option(\"header\", True).mode(\"overwrite\").csv(csv_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error saving to CSV:\", str(e))\n",
    "\n",
    "def export_to_processed(dataset_name, dataset_df):\n",
    "    destination_path = processed_folder_path + dataset_name\n",
    "    try:\n",
    "        # Write the dataset_df to the 'processed' folder\n",
    "        dataset_df.write.option(\"header\", True).mode(\"overwrite\").csv(destination_path)\n",
    "        print(f\"Dataset {dataset_name} exporteded to 'processed' folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to export dataset {dataset_name} to 'processed' folder. Error: {str(e)}\")\n",
    "    \n",
    "import_new_dataset()\n",
    "import_new_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9123fb2-8eb2-45a0-843e-b1a6a35e4652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Batch Processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

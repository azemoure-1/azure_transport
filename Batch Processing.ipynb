{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a62c64-8eb7-4fd5-8a29-ae05dbcecaeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import to_date, year, month, day, hour, minute, when, avg, regexp_replace, mean, count, round\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import col, regexp_extract, to_timestamp\n",
    "from pyspark.sql.functions import unix_timestamp, col, expr\n",
    "\n",
    "# Define Azure Blob Storage paths\n",
    "raw_folder_path = \"/mnt/public-transport-data/raw/\"\n",
    "processed_folder_path = \"/mnt/public-transport-data/processed/\"\n",
    "csv_file_path = \"/mnt/public-transport-data/processed_fils/\"\n",
    "\n",
    "# Define the schema for the CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"name_file\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Function to extract the month number from the filename\n",
    "def extract_month(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    for part in parts:\n",
    "        if part.isdigit():\n",
    "            return int(part)\n",
    "    return 0  # Return 0 if month number is not found\n",
    "\n",
    "# List to keep track of imported datasets\n",
    "imported_datasets = []\n",
    "\n",
    "# Load the existing dataset names from the CSV file\n",
    "existing_datasets = set()\n",
    "try:\n",
    "    # Read existing dataset names from the CSV file\n",
    "    existing_df = spark.read.option(\"header\", True).schema(schema).csv(csv_file_path)\n",
    "    existing_datasets = set(existing_df.select(\"name_file\").rdd.flatMap(lambda x: x).collect())\n",
    "except Exception as e:\n",
    "    print(\"Error reading existing datasets:\", str(e))\n",
    "\n",
    "# Get a list of files in the 'raw' folder in Azure Blob Storage\n",
    "files_in_raw_folder = dbutils.fs.ls(raw_folder_path)\n",
    "data_files = [file.name for file in files_in_raw_folder if file.isFile() and file.name not in existing_datasets]\n",
    "\n",
    "# Sort the data files by their numeric part (e.g., month)\n",
    "data_files.sort(key=lambda x: extract_month(x))\n",
    "\n",
    "# Function to import a new dataset\n",
    "def import_new_dataset():\n",
    "    if data_files:\n",
    "        # Get the next dataset to import\n",
    "        dataset_to_import = data_files.pop(0)\n",
    "        \n",
    "        # Check if dataset is already imported\n",
    "        if dataset_to_import in existing_datasets:\n",
    "            print(f\"Dataset {dataset_to_import} already imported. Skipping...\")\n",
    "        else:\n",
    "            # Update the list of imported datasets\n",
    "            imported_datasets.append(dataset_to_import)\n",
    "            \n",
    "            # Save dataset name and current date to the CSV file\n",
    "            save_to_csv(dataset_to_import, datetime.now().strftime('%Y-%m-%d'))\n",
    "            \n",
    "            # Display the dataset (use dataset_to_import)\n",
    "            print(f\"Displaying dataset: {dataset_to_import}\")\n",
    "            dataset_df = spark.read.option(\"header\", True).csv(raw_folder_path + dataset_to_import)\n",
    "            \n",
    "            def convert_to_24_hour(time_str):\n",
    "                return to_timestamp(time_str, 'h:mm:ss a').cast('string')\n",
    "\n",
    "            # Apply the function to DepartureTime and ArrivalTime columns\n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', regexp_extract(col('DepartureTime'), r'(\\d+:\\d+:\\d+ [APM]+)',1))\n",
    "                          .withColumn('ArrivalTime', regexp_extract(col('ArrivalTime'), r'(\\d+:\\d+:\\d+ [APM]+)', 1)))\n",
    "\n",
    "            # Convert DepartureTime and ArrivalTime to 24-hour format\n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', convert_to_24_hour(col('DepartureTime')))\n",
    "                         .withColumn('ArrivalTime', convert_to_24_hour(col('ArrivalTime'))))\n",
    "            \n",
    "            dataset_df = (dataset_df.withColumn('DepartureTime', date_format(col('DepartureTime'), 'HH:mm:ss'))\n",
    "                         .withColumn('ArrivalTime', date_format(col('ArrivalTime'), 'HH:mm:ss')))\n",
    "            \n",
    "            # Calculate the duration in seconds\n",
    "            dataset_df = dataset_df.withColumn('DurationSeconds', \n",
    "                                                (unix_timestamp(col('ArrivalTime'), 'HH:mm:ss') - \n",
    "                                                unix_timestamp(col('DepartureTime'), 'HH:mm:ss')))\n",
    "\n",
    "            # Convert the duration from seconds to HH:mm:ss format\n",
    "            dataset_df = dataset_df.withColumn('Duration', \n",
    "                                                expr(\"from_unixtime(DurationSeconds, 'HH:mm:ss')\"))\n",
    "            \n",
    "            dataset_df = dataset_df.withColumn('DelayCategory',\n",
    "                                        when(col('Delay') == 0, 'Pas de Retard')\n",
    "                                        .when((col('Delay') >= 1) & (col('Delay') <= 10), 'Retard Court')\n",
    "                                        .when((col('Delay') >= 11) & (col('Delay') <= 20), 'Retard Moyen')\n",
    "                                        .when(col('Delay') > 20, 'Long Retard')\n",
    "                                        .otherwise('Unknown'))\n",
    "            \n",
    "\n",
    "            peak_passenger_threshold = 70\n",
    "\n",
    "            # Identify peak and off-peak hours based on the number of passengers\n",
    "            dataset_df = (dataset_df.withColumn('Hour', hour(col('DepartureTime')))\n",
    "                                      .withColumn('HourCategory',\n",
    "                                                  when(col('Passengers') >= peak_passenger_threshold, 'Peak Hour')\n",
    "                                                  .otherwise('Off-Peak Hour')))\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            dataset_df.show()  # Show the contents of the dataset\n",
    "            \n",
    "\n",
    "            \n",
    "            # Copy the dataset to the 'processed' folder\n",
    "            copy_to_processed(dataset_to_import, dataset_df)\n",
    "            \n",
    "            # TODO: Import the dataset (use dataset_to_import)\n",
    "            # Replace the following print statement with your dataset import logic\n",
    "            print(f\"Imported dataset: {dataset_to_import}\")\n",
    "    else:\n",
    "        print(\"No more datasets to import.\")\n",
    "\n",
    "# Function to save dataset name and date to the CSV file\n",
    "def save_to_csv(dataset_name, import_date):\n",
    "    try:\n",
    "        # Create a DataFrame with the new dataset import record\n",
    "        new_record_df = spark.createDataFrame([(dataset_name, import_date)], schema)\n",
    "        \n",
    "        # Read existing dataset names and dates from the CSV file\n",
    "        existing_df = spark.read.option(\"header\", True).schema(schema).csv(csv_file_path)\n",
    "        \n",
    "        # Union the existing dataset with the new record\n",
    "        updated_df = existing_df.union(new_record_df)\n",
    "        \n",
    "        # Write the updated DataFrame to the CSV file\n",
    "        updated_df.write.option(\"header\", True).mode(\"overwrite\").csv(csv_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error saving to CSV:\", str(e))\n",
    "\n",
    "def copy_to_processed(dataset_name, dataset_df):\n",
    "    destination_path = processed_folder_path + dataset_name\n",
    "    try:\n",
    "        # Write the dataset_df to the 'processed' folder\n",
    "        dataset_df.write.option(\"header\", True).mode(\"overwrite\").csv(destination_path)\n",
    "        print(f\"Dataset {dataset_name} copied to 'processed' folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy dataset {dataset_name} to 'processed' folder. Error: {str(e)}\")\n",
    "    \n",
    "import_new_dataset()\n",
    "import_new_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9123fb2-8eb2-45a0-843e-b1a6a35e4652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Batch Processing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
